"""About â€” Project information, architecture, and credits."""
import streamlit as st

st.set_page_config(page_title="About", page_icon="â„¹ï¸", layout="wide")

st.title("â„¹ï¸ About This Project")
st.markdown("---")

# â”€â”€ Overview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown(
    """
    ### â˜• The One With The AI

    An end-to-end NLP project that uses **Word2Vec embeddings** and
    **Sentence-BERT** trained on Friends TV show scripts to power semantic
    search and personality matching.

    This project demonstrates the full NLP pipeline â€” from raw text preprocessing
    to deployed interactive features â€” using a fun, relatable dataset.
    """
)

# â”€â”€ Architecture â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("### ğŸ—ï¸ Architecture")

st.markdown(
    """
    ```
    Raw Scripts (.txt)
        â”‚
        â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Preprocessing         â”‚  spaCy + NLTK stopwords
    â”‚   (cleaning, tokenizing,â”‚  Selective stopword removal
    â”‚    lemmatization)        â”‚  Speaker normalization
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Word2Vec (Hybrid)     â”‚  GloVe-300D pre-trained base
    â”‚   Fine-tuned on         â”‚  + 30 epochs fine-tuning
    â”‚   Friends corpus        â”‚  7,090 vocab, 93.3% coverage
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
        â–¼         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Search â”‚ â”‚ Personality Matchâ”‚
    â”‚ (W2V)  â”‚ â”‚ (3-Signal Blend) â”‚
    â”‚        â”‚ â”‚ SBERT + W2V +    â”‚
    â”‚        â”‚ â”‚ Topic Boosting   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚              â”‚
        â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Streamlit  â”‚
         â”‚ Frontend   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
    """
)

# â”€â”€ Tech Stack â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("### ğŸ› ï¸ Technology Stack")

col1, col2 = st.columns(2)
with col1:
    st.markdown(
        """
        | Layer | Technology |
        |---|---|
        | **Embedding Model** | Gensim Word2Vec (Skip-gram) |
        | **Pre-trained Base** | GloVe Wiki-Gigaword 300D |
        | **Sentence Embeddings** | Sentence-BERT (all-MiniLM-L6-v2) |
        | **Text Processing** | spaCy, NLTK |
        """
    )
with col2:
    st.markdown(
        """
        | Layer | Technology |
        |---|---|
        | **ML** | scikit-learn |
        | **Visualization** | Altair, PyNarrative |
        | **Frontend** | Streamlit |
        | **Data** | Pandas, NumPy |
        """
    )

# â”€â”€ Key Design Decisions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("### ğŸ¯ Key Design Decisions")

st.markdown(
    """
    1. **Hybrid Word2Vec** â€” Pure training on ~60K lines would produce weak embeddings.
       We start with GloVe (400K words) and fine-tune on Friends scripts for
       domain-specific vocabulary.

    2. **3-Signal Personality Blend** â€” Word2Vec centroids alone collapse because
       characters discuss similar topics. We combine:
       - **SBERT** (50%) â€” contextual sentence understanding
       - **Word2Vec** (20%) â€” Friends-specific patterns
       - **Topic Boosting** (30%) â€” curated character-keyword associations

    3. **NLTK Stopwords** â€” Comprehensive removal (~198 words) with sentiment words
       preserved (not, no, contractions) for personality matching accuracy.

    4. **Discriminative Centroids** â€” Instead of raw averaging (which causes centroid collapse),
       we subtract the global mean to isolate each character's *distinctive* direction.
    """
)

# â”€â”€ Dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("### ğŸ“Š Dataset Stats")

c1, c2, c3, c4 = st.columns(4)
with c1:
    st.metric("Dialogue Lines", "60,961")
with c2:
    st.metric("Main Character Lines", "49,208")
with c3:
    st.metric("Episodes", "228")
with c4:
    st.metric("Unique Speakers", "801")

# â”€â”€ Credits â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown("---")
st.markdown(
    """
    ### ğŸ“ Credits

    - **Dataset**: Friends TV Show Script (Kaggle)
    - **Pre-trained Embeddings**: GloVe (Stanford NLP)
    - **Sentence Transformer**: all-MiniLM-L6-v2 (Hugging Face)
    - **Built by**: Rishabh

    ---
    *Made with â˜• and NLP*
    """
)
